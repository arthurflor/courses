{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW03.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurflor23/computer-vision/blob/master/HW03/HW03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKUWoKtwBSIB",
        "colab_type": "text"
      },
      "source": [
        "# Assignment#3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3fxcTbiA90z",
        "colab_type": "text"
      },
      "source": [
        "In this assignment you will practice putting together a simple image classification pipeline, based on the k-Nearest Neighbor or the SVM/Softmax classifier. Once you have the starter code (regardless of which method you choose above), you will need to download the CIFAR-10 dataset.\n",
        "\n",
        "**NOTES**\n",
        "1. The assignment#3 code has been tested to be compatible with python version 3.7. You will need to make sure that during your virtual environment setup that the correct version of python is used. You can confirm your python version by (1) activating your virtualenv and (2) running which python.\n",
        "\n",
        "2. Feel free to pick any others ways to implement this assignment#3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5930ZSh0mwq",
        "colab_type": "text"
      },
      "source": [
        "## Image Classifier (CIFAR-10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P33NlKae0yA_",
        "colab_type": "text"
      },
      "source": [
        "First, install TensorFlow 2.0:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Zy0DD2RIyb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "592c9ea2-1bb5-4cca-a765-1c2a45a6687e"
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 332.1MB 58kB/s \n",
            "\u001b[K     |████████████████████████████████| 419kB 51.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 28.9MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-FQoU6X04zR",
        "colab_type": "text"
      },
      "source": [
        "Setup the training dataset using the Dataset API in TensorFlow and extracting the CIFAR10 data from the Keras datasets library:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYSg7o2E05Tt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "19b8faea-f27e-4d5c-b5e9-fcf2db8bb084"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# treatment to the train dataset, with batch size 32 and random distortion of the images:\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32).shuffle(10000)\n",
        "train_dataset = train_dataset.map(lambda x, y: (tf.math.divide(tf.cast(x, tf.float32), 255.0), tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n",
        "train_dataset = train_dataset.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# treatment to the validation dataset, with a larger batch size and no random distortion of the images\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(1000).shuffle(10000)\n",
        "valid_dataset = valid_dataset.map(lambda x, y: (tf.math.divide(tf.cast(x, tf.float32),255.0), tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n",
        "valid_dataset = valid_dataset.repeat()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH1CdWKm2Qmb",
        "colab_type": "text"
      },
      "source": [
        "Create the convolutional neural network to classify the images:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpSlPOz92Rzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CIFAR10Model(tf.keras.Model):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CIFAR10Model, self).__init__(name='cifar_cnn')\n",
        "        \n",
        "        self.conv1 = tf.keras.layers.Conv2D(64, 5, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001))\n",
        "        self.max_pool2d = tf.keras.layers.MaxPooling2D((3, 3), (2, 2), padding='same')\n",
        "        self.max_norm = tf.keras.layers.BatchNormalization()\n",
        "        self.conv2 = tf.keras.layers.Conv2D(64, 5, padding='same', activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001))\n",
        "\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "\n",
        "        self.fc1 = tf.keras.layers.Dense(750, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l=0.001))\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.fc2 = tf.keras.layers.Dense(10)\n",
        "        self.softmax = tf.keras.layers.Softmax()\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.max_pool2d(self.conv1(x))\n",
        "        x = self.max_norm(x)\n",
        "        x = self.max_pool2d(self.conv2(x))\n",
        "        x = self.max_norm(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14fDW9iV4Ir_",
        "colab_type": "text"
      },
      "source": [
        "Instantiate the model class, compile and run the training.\n",
        "\n",
        "**Note**: Also added an early stopping with 5 patience for the *val_loss*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UjDXYcV4POP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "55038298-cec2-45eb-e95d-f74a969174a6"
      },
      "source": [
        "model = CIFAR10Model()\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
        "                                              min_delta=0.001,\n",
        "                                              patience=5,\n",
        "                                              restore_best_weights=True, \n",
        "                                              verbose=1)]\n",
        "\n",
        "model.fit(train_dataset,\n",
        "          epochs=1000,\n",
        "          steps_per_epoch=1500, \n",
        "          validation_data=valid_dataset,\n",
        "          validation_steps=3,\n",
        "          callbacks=callbacks,\n",
        "          shuffle=True,\n",
        "          verbose=1)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "1500/1500 [==============================] - 25s 17ms/step - loss: 2.4422 - accuracy: 0.3812 - val_loss: 11.3696 - val_accuracy: 0.1377\n",
            "Epoch 2/1000\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 1.6970 - accuracy: 0.5812 - val_loss: 3.0047 - val_accuracy: 0.2493\n",
            "Epoch 3/1000\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 1.5668 - accuracy: 0.6403 - val_loss: 3.1246 - val_accuracy: 0.2020\n",
            "Epoch 4/1000\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 1.4901 - accuracy: 0.6706 - val_loss: 3.5717 - val_accuracy: 0.2023\n",
            "Epoch 5/1000\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 1.4457 - accuracy: 0.6803 - val_loss: 4.4993 - val_accuracy: 0.2010\n",
            "Epoch 6/1000\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 1.4121 - accuracy: 0.6937 - val_loss: 2.7962 - val_accuracy: 0.3430\n",
            "Epoch 7/1000\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 1.3776 - accuracy: 0.7069 - val_loss: 2.7806 - val_accuracy: 0.3590\n",
            "Epoch 8/1000\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 1.3577 - accuracy: 0.7076 - val_loss: 2.3518 - val_accuracy: 0.3933\n",
            "Epoch 9/1000\n",
            "1500/1500 [==============================] - 19s 13ms/step - loss: 1.3329 - accuracy: 0.7137 - val_loss: 2.5731 - val_accuracy: 0.3863\n",
            "Epoch 10/1000\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 1.3219 - accuracy: 0.7144 - val_loss: 3.7734 - val_accuracy: 0.2727\n",
            "Epoch 11/1000\n",
            "1500/1500 [==============================] - 18s 12ms/step - loss: 1.3093 - accuracy: 0.7206 - val_loss: 2.5392 - val_accuracy: 0.4370\n",
            "Epoch 12/1000\n",
            "1500/1500 [==============================] - 19s 13ms/step - loss: 1.2827 - accuracy: 0.7220 - val_loss: 2.5543 - val_accuracy: 0.4000\n",
            "Epoch 13/1000\n",
            "1498/1500 [============================>.] - ETA: 0s - loss: 1.2906 - accuracy: 0.7283Restoring model weights from the end of the best epoch.\n",
            "1500/1500 [==============================] - 19s 13ms/step - loss: 1.2903 - accuracy: 0.7283 - val_loss: 4.3799 - val_accuracy: 0.2683\n",
            "Epoch 00013: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f54c7e90320>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    }
  ]
}